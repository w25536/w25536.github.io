---
title: "AI 심화 day6"
last_modified_at: 2025-01-23
categories:
  - 부트캠프
tags:
  - python
  - 파이썬
  - torchtext
  - pytorch
  - 파이토치
  - 전처리
  - data science
  - 데이터 분석
  - 딥러닝
  - 딥러닝 자격증
  - 머신러닝
  - 빅데이터
excerpt: "AI 심화 day6 정리"
use_math: true
classes: wide
---
## LSTM (Long Short-Term Memory) 이해

### LSTM의 기본 개념

LSTM은 RNN의 장기 의존성 문제를 해결하기 위해 개발된 순환 신경망의 한 종류입니다. 기존 RNN에서 발생하는 기울기 소실(Vanishing Gradient) 문제를 해결하여 장기간의 정보를 효과적으로 기억할 수 있습니다.

### 4.4. LSTM 작동 과정 (중요)

LSTM의 핵심은 **셀 상태(Cell State)**와 **게이트(Gate) 메커니즘**입니다:

1. **망각 게이트(Forget Gate)**: 이전 셀 상태에서 어떤 정보를 버릴지 결정
2. **입력 게이트(Input Gate)**: 새로운 정보 중 어떤 것을 셀 상태에 저장할지 결정
3. **출력 게이트(Output Gate)**: 셀 상태를 바탕으로 어떤 부분을 출력할지 결정

### 4.6 Peephole LSTM

Peephole LSTM은 기본 LSTM에서 게이트들이 셀 상태를 직접 참조할 수 있도록 개선된 버전입니다. 이를 통해 더 정확한 게이트 제어가 가능합니다.

## GRU (Gated Recurrent Unit)

GRU는 LSTM을 단순화한 모델로, 게이트 수를 줄여 계산 효율성을 높인 구조입니다:
- **리셋 게이트(Reset Gate)**: 이전 은닉 상태의 영향력 조절
- **업데이트 게이트(Update Gate)**: 새로운 정보와 이전 정보의 비율 조절

## 신경망 회로도 이해

### 가중치 매개변수의 특성

- **(W, U) 가중치**: 시간에 의존하지 않는 고정된 매개변수
  - W: 입력-은닉층 간 가중치
  - U: 은닉-은닉층 간 가중치 (순환 연결)

### 정보 저장 메커니즘

- **시점 정보 저장**: 각 시점의 정보가 은닉 상태에 누적되어 저장
- **과거 정보 보존**: 게이트 메커니즘을 통해 중요한 과거 정보를 선택적으로 유지
- **장기 의존성**: 멀리 떨어진 시점의 정보도 효과적으로 활용 가능
