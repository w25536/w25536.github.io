---
title: "오픈AI, 고성능 오픈‑웨이트 모델 2종(gpt‑oss‑120B·gpt‑oss‑20B) 출시"
last_modified_at: 2025-08-06
categories:
  - 
tags:
  - OpenAI
  - gpt-oss-120B
  - gpt-oss-20B
  - 오픈소스
  - 오픈웨이트
  - MoE
  - 전문가혼합
  - Apache2.0
  - HuggingFace
  - 벤치마크
  - 추론성능
  - 코딩모델
  - STEM
  - 안전성
  - 미세조정
  - 로컬실행
  - 엣지디바이스
excerpt: "오픈AI, 고성능 오픈‑웨이트 모델 2종(gpt‑oss‑120B·gpt‑oss‑20B) 출시"
use_math: true
classes: wide
---


![](https://velog.velcdn.com/images/u25536/post/85305575-377f-4ff3-b391-f7c5a4544aa3/image.png)

HLE 벤치마크 결과 (사진=오픈AI)


**핵심 정리**

- **오픈AI, 오픈 웨이트 모델 두 종류 공개**  
  - **gpt‑oss‑120B**: 120 B 파라미터 모델, 80 GB GPU 한 대에서 실행 가능, 노트북에서도 로컬 구동 가능.  
  - **gpt‑oss‑20B**: 20 B 파라미터 모델, 16 GB 메모리(엣지 디바이스·휴대폰)에서도 실행.

- **라이선스·배포**  
  - Apache 2.0 라이선스 적용 → 소스·수정 내용 고지 시 상업적 사용 가능.  
  - Hugging Face를 통해 배포, API·플레이그라운드·미세조정 가이드 제공.

- **모델 구조·효율성**  
  - MoE(전문가 혼합) 방식 사용 → gpt‑oss‑120B는 5.1 B, gpt‑oss‑20B는 3.6 B 파라미터를 활성화(전문가 모델은 각각 117 B/21 B).  
  - 그룹화된 멀티‑쿼리 어텐션, 희소·덴스 어텐션 교대 적용, 컨텍스트 창 128 k 토큰.

- **학습·정제**  
  - 고품질 영어 STEM·코딩·일반 지식 데이터셋 + “o200k_harmony” 토크나이저 사용.  
  - 지도 학습 + 고연산 RL(CoT·도구 사용) → 안전·정렬을 강화한 사후 훈련 진행.

- **벤치마크·성능**  
  - **gpt‑oss‑120B**: 주요 추론 벤치마크에서 o4‑mini 수준, 코딩(Codeforces 2622점), 일반 문제(MMLU·HLE), 도구 호출(Tau), 의료·수학(AIME) 등에서 o4‑mini와 동등 혹은 우수.  
  - **gpt‑oss‑20B**: o3‑mini 수준에 근접, 작은 규모에도 코딩·수학·의료에서 o3‑mini 이상 성능.  
  - 모두 중국 오픈소스(DeepSeek·Qwen)보다 우수하지만, OpenAI의 최신 ‘o3’·‘o4‑mini’보다는 약간 뒤처짐.

- **안전·악용 방지**  
  - 사전 훈련 단계에서 화학·생물·핵 등 위험 데이터 제외.  
  - 사후 정렬·지침 학습으로 안전하지 않은 프롬프트 거부·프롬프트 추출 방어.  
  - 모델 미세조정 테스트를 거쳐 악의적 사용을 차단하는 버전 제공.

- **배포·생태계**  
  - Hugging Face 외 Azure, AWS, Together AI, Databricks 등 주요 클라우드·플랫폼과 협업.  
  - NVIDIA, AMD, Cerebras, Groq 등 다양한 하드웨어 최적화 지원.

- **오픈AI 목표·CEO 발언**  
  - “개방형 모델 중 최고 성능·접근성 제공”이 목표.  
  - 저예산·리소스 제한 환경·소규모 조직에 AI 기회 확대 강조.  
  - 샘 알트먼, “o4‑mini 수준 성능을 노트북·휴대폰에서 구현, 큰 승리”라고 트위터에 언급.  