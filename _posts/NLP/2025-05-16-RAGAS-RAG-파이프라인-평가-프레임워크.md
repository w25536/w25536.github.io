---
title: RAGAS :RAG 파이프라인 평가 프레임워크
last_modified_at: 2025-05-16
categories:
  - NLP
  - RAG
  - Evaluation
tags:
  - RAGAS
  - RAG
  - Evaluation
  - Metrics
  - Faithfulness
  - Answer
  - Relevancy
  - Context
  - Recall
  - Context
  - Precision
excerpt: "RAGAS : RAG 파이프라인 평가 프레임워크"
use_math: true
classes: wide
---
# RAG 파이프라인 성능 평가


필자는 LLM 개발 과정에 RAG 결과가 좋아 RAG 성능 테스트를 하지 않았지만, RAG 성능을 확인해보라는 추천을 받아 다음과 같이 정리해 보았다. 다음 내용은 다른 블로그에서도 가져오고 여러 가지 짬뽕시킨 내용이다.

## 개요

머신러닝 태스크에서 모델 또는 파이프라인을 만들고 나면 성능을 평가합니다.
RAG(Retrieval-Augmented Generation) 파이프라인은 Retrieval과 Generation 두 측면에서 평가하게 됩니다.

## RAGAS 소개

* **RAGAS**: RAG의 Rule Base 지표를 자동 계산하는 프레임워크
* **기능**: input/output을 이용해 평가 지표 계산에 필요한 데이터셋 (`question`, `answer`, `contexts`, `ground_truth`) 생성 → `evaluate()` 함수 한 줄로 자동 계산

공식문서: [공식문서](https://github.com/openlab-lab/ragas)
GitHub: [RAGAS GitHub](https://github.com/openlab-lab/ragas)
블로그: [Evaluate RAG Pipeline using RAGAS](https://medium.aiplanet.com/evaluate-rag-pipeline-using-ragas-fbdd8dd466c1)

## 사용 방법

### 1. 평가지표 계산용 데이터셋 준비

```python
from datasets import Dataset

data_samples = {
    'question': [...],
    'answer': [...],
    'contexts': [...],
    'ground_truth': [...]
}

dataset = Dataset.from_dict(data_samples)
```

### 2. 평가지표 계산

```python
from ragas import evaluate

# 원하는 메트릭 리스트 지정 (예: faithfulness)
score = evaluate(dataset, metrics=[faithfulness, answer_relevancy, ...])
print(score.to_pandas())
```

## RAGAS 주요 성능 메트릭

### Retrieval

* **Context recall**: 필요한 맥락 정보를 검색했는가?
* **Context precision**: 관련 있는 문서가 상위에 랭크되었는가?

### Generation

* **Faithfulness**: 생성된 답변이 맥락에 근거한 정확한 답변인가?
* **Answer relevancy**: 질문에 얼마나 관련성이 있는가?
* **Answer correctness**: 답변의 정답 여부

## Quickstart + 메트릭 예제

```python
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import (
    answer_relevancy,
    context_precision,
    faithfulness,
    context_recall,
    answer_correctness
)
import os

os.environ["OPENAI_API_KEY"] = "sk-..."

data_samples = {
    'question': [
        'When was the first super bowl?',
        'Who won the most super bowls?'
    ],
    'answer': [
        'The first superbowl was held on Jan 15, 1967',
        'The most super bowls have been won by The New England Patriots'
    ],
    'contexts': [
        ['The First AFL–NFL World Championship Game was...'],
        ['The Green Bay Packers...','The Packers compete...']
    ],
    'ground_truth': [
        'The first superbowl was held on January 15, 1967',
        'The New England Patriots have won the Super Bowl a record six times'
    ]
}

dataset = Dataset.from_dict(data_samples)

score = evaluate(
    dataset,
    metrics=[answer_relevancy, context_precision, faithfulness, context_recall, answer_correctness]
)
print(score)
```

**예시 출력**

```
{'answer_relevancy': 0.9592,
 'context_precision': 0.5000,
 'faithfulness': 0.5000,
 'context_recall': 0.5000,
 'answer_correctness': 0.8651}
```

## 튜토리얼: RAG 파이프라인 구축 및 평가

### 데이터 로드 및 청킹

```python
from langchain.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter

loader = TextLoader('./state_of_the_union.txt')
documents = loader.load()

text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
chunks = text_splitter.split_documents(documents)
```

### RAG 파이프라인 구성

```python
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import RunnablePassthrough
from langchain.output_parsers import StrOutputParser

llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)
template = """You are an assistant for question-answering tasks.  
Use the following pieces of retrieved context to answer the question.  
If you don't know the answer, just say that you don't know.  
Use two sentences maximum and keep the answer concise.
Question: {question}
Context: {context}
Answer:
"""
prompt = ChatPromptTemplate.from_template(template)

rag_chain = (
    {"context": retriever, "question": RunnablePassthrough()}  
    | prompt  
    | llm
    | StrOutputParser()
)
```

### RAGAS dataset 생성 및 평가

```python
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import (
    context_precision,
    context_recall,
    faithfulness,
    answer_relevancy,
)

questions = [...]
ground_truths = [...]
answers, contexts = [], []

for q in questions:
    answers.append(rag_chain.invoke(q))
    contexts.append([doc.page_content for doc in retriever.get_relevant_documents(q)])

data = {
    'question': questions,
    'answer': answers,
    'contexts': contexts,
    'ground_truth': ground_truths
}
dataset = Dataset.from_dict(data)

result = evaluate(dataset, metrics=[
    context_precision,
    context_recall,
    faithfulness,
    answer_relevancy
])
print(result.to_pandas())
```
