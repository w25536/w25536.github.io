{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.5)\n",
      "Path to dataset files: /Users/jeongho/.cache/kagglehub/datasets/edumagalhaes/quality-prediction-in-a-mining-process/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"edumagalhaes/quality-prediction-in-a-mining-process\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "\n",
    "df = pd.read_csv(os.path.join(path, \"MiningProcess_Flotation_Plant_Database.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cols in df.columns:\n",
    "    df[cols] = df[cols].apply(lambda x: x.replace(\",\", \".\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         2017-03-10 01:00:00\n",
       "1         2017-03-10 01:00:00\n",
       "2         2017-03-10 01:00:00\n",
       "3         2017-03-10 01:00:00\n",
       "4         2017-03-10 01:00:00\n",
       "                 ...         \n",
       "737448    2017-09-09 23:00:00\n",
       "737449    2017-09-09 23:00:00\n",
       "737450    2017-09-09 23:00:00\n",
       "737451    2017-09-09 23:00:00\n",
       "737452    2017-09-09 23:00:00\n",
       "Name: date, Length: 737453, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input(df):\n",
    "    df.copy()\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])  # adding year and date columns\n",
    "    df[\"month\"] = df[\"date\"].dt.month\n",
    "\n",
    "    df = df.drop([\"date\"], axis=1)\n",
    "\n",
    "    df = df.astype(float)\n",
    "\n",
    "    corr = df.corr()\n",
    "    corr_cols = corr[abs(corr[\"% Silica Concentrate\"]) > 0.1].index.tolist()\n",
    "\n",
    "    df = df[corr_cols]\n",
    "\n",
    "    y = df[\"% Silica Concentrate\"]\n",
    "    X = df.drop([\"% Silica Concentrate\"], axis=1)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the preprocess_input function to include log transformation\n",
    "def preprocess_input2(df):\n",
    "    df = df.copy()\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    df[\"month\"] = df[\"date\"].dt.month\n",
    "\n",
    "    df = df.drop([\"date\"], axis=1)\n",
    "    df = df.astype(float)\n",
    "\n",
    "    skew_df = pd.DataFrame(df.columns, columns=[\"feature\"])\n",
    "    skew_df[\"skew\"] = abs(scipy.stats.skew(df))\n",
    "    skew_df[\"skew_check\"] = skew_df[\"skew\"] > 0.5\n",
    "\n",
    "    # Get features that need transformation\n",
    "    features_to_transform = skew_df[skew_df[\"skew_check\"]][\"feature\"].tolist()\n",
    "\n",
    "    # Apply log1p transformation to highly skewed features\n",
    "    for feature in features_to_transform:\n",
    "        df[feature] = np.log1p(df[feature])\n",
    "\n",
    "    corr = df.corr()\n",
    "    corr_cols = corr[abs(corr[\"% Silica Concentrate\"]) > 0.1].index.tolist()\n",
    "\n",
    "    df = df[corr_cols]\n",
    "\n",
    "    y = df[\"% Silica Concentrate\"]\n",
    "    X = df.drop([\"% Silica Concentrate\"], axis=1)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = preprocess_input2(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6809614917814494"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
